---
title: 机器学习聚类算法总结
comments: true
toc: true
mathjax2: true
categories: [artificial-intelligence,machine-learning]
tags: [machine-learning,Clustering]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
date: 2016-12-29 15:43:27
---
聚类算法是一种无监督学习，无训练样本，根据信息相似度原则进行聚类，通过聚类，人们能够识别密集的和稀疏的区域，因而发现全局的分布模式，以及数据属性之间的关系
 <!--more-->
## 聚类
### Kmeans
原理：使各个样本与所在簇的质心的均值的误差平方和达到最小
步骤：
1. 随机在图中取K（这里K=2）个种子点。
2. 然后对图中的所有点求到这K个种子点的距离，假如点Pi离种子点Si最近，那么Pi属于Si点群
3. 接下来，我们要移动种子点到属于他的“点群”的中心。
4. 然后重复第2）和第3）步，直到，种子点没有移动。

优点：
1. 原理简单
2. 容易解释

缺点：
1. k值需要事先指定，但k值往往是难以估计的
2. 最终状态跟初始值有关（用kmeans++算法解决）
3. 对异常值比较敏感
4. 收敛太慢
5. 结果为局部最优
6. 比层次聚类法运算量小, 适用于小到中大规模样本数据
7. 适用于发现球状类
8. 针对数据分布呈非圆形的情况下不适应
9. 没有考虑数据的先验分布
10. 通常采用欧式距离计算相似性，有很大局限性


### 层次聚类(Hierarchical Clustering)
通过计算不同类别数据点间的相似度来创建一棵有层次的嵌套聚类树。在聚类树中，不同类别的原始数据点是树的最低层，树的顶层是一个聚类的根节点。

创建聚类树有自下而上合并和自上而下分裂两种方法
其中合并法：通过计算两类数据点间的相似性，对所有数据点中最为相似的两个数据点进行组合，并反复迭代这一过程。简单的说层次聚类的合并算法是通过计算每一个类别的数据点与所有数据点之间的距离来确定它们之间的相似性，距离越小，相似度越高。并将距离最近的两个数据点或类别进行组合，生成聚类树。

距离计算：采用欧几里得距离

先计算数据点之间的距离

再计算数据点与组合点之间的距离

最后计算组合点与组合点之间的距离

层次聚类树状图
![Hierarchical-Clustering](/img/Hierarchical-Clustering.png)


### 基于密度的聚类

### 谱聚类(Spectral Clustering, SC)
描述：一种基于图论的聚类方法——将带权无向图划分为两个或两个以上的最优子图，使子图内部尽量相似，而子图间距离尽量距离较远，以达到常见的聚类的目的。其中的最优是指最优目标函数不同，可以是割边最小分割——如图1的Smallest cut(如后文的Min cut)， 也可以是分割规模差不多且割边最小的分割——如图1的Best cut(如后文的Normalized cut)。
![谱聚类无向图划分](/img/谱聚类无向图划分.jpg)
这样，谱聚类能够识别任意形状的样本空间且收敛于全局最优解，其基本思想是利用样本数据的相似矩阵(拉普拉斯矩阵)进行特征分解后得到的特征向量进行聚类。

准则：紧密性，连通性。
思想：通过讲数据映射到一个具有约化的维度的新空间，可使得相似性显而易见。谱聚类正是通过拉普拉斯特征映射，实现了这一目标。也就是，利用样本数据之间的相似矩阵（拉普拉斯矩阵）进行特征分解（ 通过Laplacian Eigenmap 的降维方式降维），然后将得到的特征向量进行 K-means聚类。

如何切割图则成为问题的关键。
目标函数如下
#### 最小切
将图分成两个部分A,B,从而使连接A与B的边权值加和最小。
![最小切](/img/mincut.png)

#### 比例割(Ratio cut)
![比例割](/img/ratiocut.png)

#### 规范割(Normalized cut)
![规范割(Normalized cut)](/img/ncut.png)

这是谱聚类的目标，式子虽然简单，但是要最小化它却是一个 NP 难问题，不方便求解，为了找到解决办法，让我们先来做做变形。
令 V 表示 Graph 的所有节点的集合，首先定义一个 N 维向量 f：
![fi](/img/fi.png)
的矩阵 L=D-W ,这个 L 有一个性质就是：
![](/img/flf.png)
这个是对任意向量 f 都成立的，很好证明，只要按照定义展开就可以得到了。把我们刚才定义的那个 f 带进去，就可以得到
![](/img/fif2.png)
因此最小化 RatioCut 就等价于最小化 f'Lf 。

#### 步骤
核心步骤：
一、在原始空间中定义局部邻域。然后对相同领域内的实例，定义与实例之间的距离成反比的相似度度量，在这种拉普拉斯映射下，把实例安置在新空间
二、在新空间运行k均值聚类

其中拉普拉斯矩阵定义为：L = D - W,其中D为图的度矩阵，W为图的邻接矩阵。

算法流程
1. 根据数据构造一个 Graph ，Graph 的每一个节点对应一个数据点，将相似的点连接起来，并且边的权重用于表示数据之间的相似度。把这个 Graph 用邻接矩阵的形式表示出来，记为 W 。一个最偷懒的办法就是：直接用我们前面在 K-medoids 中用的相似度矩阵作为 W 。
2. 把 W 的每一列元素加起来得到 N 个数，把它们放在对角线上（其他地方都是零），组成一个 N × N 的矩阵，记为 D 。并令 L = D-W 。
3. 求出 L 的前 k 个特征值（在本文中，除非特殊说明，否则“前 k 个”指按照特征值的大小从小到大的顺序）以及对应的特征向量 。
4. 把这 k 个特征（列）向量排列在一起组成一个 N × k 的矩阵，将其中每一行看作 k 维空间中的一个向量，并使用 K-means 算法进行聚类。聚类的结果中每一行所属的类别就是原来 Graph 中的节点亦即最初的 N 个数据点分别所属的类别

特性：
1. 谱聚类和传统的聚类方法（例如 K-means）相比，谱聚类只需要数据之间的相似度矩阵就可以了，而不必像K-means那样要求数据必须是 N 维欧氏空间中的向量。
2. 对于不规则的误差数据不是那么敏感，而且 performance 也要好一些。
3. 计算复杂度比 K-means 要小，特别是在像文本数据或者平凡的图像数据这样维度非常高的数据上运行的时候。
