---
title: 自然语言处理之入门概念解析
comments: true
toc: true
mathjax2: true
date: 2017-11-23 14:00:35
categories: [artificial-intelligence,NLP]
tags: [deep-learning,Neural Networks]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
---
自然语言处理（NLP）是指机器理解并解释人类写作、说话方式的能力。它的目标是让计算机／机器在理解语言上像人类一样智能。最终目标是弥补人类交流（自然语言）和计算机理解（机器语言）之间的差距。
 <!--more-->
系列步骤：
## 分词
### 基于词典分词算法也称字符串匹配分词算法。
该算法是按照一定的策略将待匹配的字符串和一个已建立好的“充分大的”词典中的词进行匹配，若找到某个词条，则说明匹配成功，识别了该词。
常见的基于词典的分词算法分为以下几种：正向最大匹配法、逆向最大匹配法和双向匹配分词法等。基于词典的分词算法是应用最广泛、分词速度最快的。很长一段时间内研究者都在对基于字符串匹配方法进行优化，比如最大长度设定、字符串存储和查找方式以及对于词表的组织结构，比如采用TRIE索引树、哈希索引等。

### 基于统计的机器学习算法
这类目前常用的是算法是HMM、CRF、SVM、深度学习等算法，比如stanford、Hanlp分词工具是基于CRF算法。以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。
常见的分词器都是使用机器学习算法和词典相结合，一方面能够提高分词准确率，另一方面能够改善领域适应性。

## 词表示
### one-hot
NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。

### 分布式表示
基于分布假说的词表示方法，根据建模的不同，主要可以分为三类：
1. 基于矩阵的分布表示
>基于矩阵的分布表示通常又称为分布语义模型，在这种表示下，矩阵中的一行，就成为了对应词的表示，这种表示描述了该词的上下文的分布。由于分布假说认为上下文相似的词，其语义也相似，因此在这种表示下，两个词的语义相似度可以直接转化为两个向量的空间距离。        
常见到的Global Vector 模型（ GloVe模型）是一种对“词-词”矩阵进行分解从而得到词表示的方法，属于基于矩阵的分布表示。

2. 基于聚类的分布表示
3. 基于神经网络的分布表示
>基于神经网络的分布表示一般称为词向量、词嵌入（ word embedding）或分布式表示（ distributed representation）。

尽管这些不同的分布表示方法使用了不同的技术手段获取词表示，但由于这些方法均基于分布假说，它们的核心思想也都由两部分组成：一、选择一种方式描述上下文；二、选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系。

## 词嵌入（word embedding）
由one-hot稀疏到【低维、实数】的空间表示
传统自然语言处理的表达，一般以词为单位。自然语言字符表达成计算机能处理的信息，采取离散表达。假设有个词表，可以理解为字典，如何在这一万个字中表达这一个字。用一万维的向量表达这个字，某个字只可能在某一维地方为1，其他位置都为0。这种方法特点：根据词表大小决定向量长度。很占空间，即使稀疏表示。缺乏泛化性，即 lack of generalization。这种表示没法表达相似（cos函数）。
所以希望能够有一种方式计算这种泛化性，稠密向量。那就是词嵌入（word embedding）
具体是什么东西呢？

### word2vec
处理 word embedding 的常用技术。
Word2Vec 是代码项目的名字，只是计算 word embedding 的一个工具，是 CBOW 和 Skip-Gram 这两个模型开源出来的工具。连续实值词表达也叫词嵌入 word embedding。CBOW 是利用词的上下文预测当前的单词；而 Skip-Gram 则是利用当前词来预测上下文。
1. CBOW
2. skip-gram

## 语言模型
基于统计：N-gram
N-Gram是一种基于统计语言模型的算法。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。
每一个字节片段称为gram，对所有gram的出现频度进行统计，并且按照事先设定好的阈值进行过滤，形成关键gram列表，也就是这个文本的向量特征空间，列表中的每一种gram就是一个特征向量维度。

该模型基于这样一种假设，第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。


## 应用领域
### 自然语言理解（NLU)
NLU 是要理解给定文本的含义。文本内每个单词的特性与结构需要被理解。在理解结构上，NLU 要理解自然语言中的以下几个歧义性：
词法歧义性：单词有多重含义
句法歧义性：语句有多重解析树
语义歧义性：句子有多重含义
回指歧义性（Anaphoric Ambiguity）：之前提到的短语或单词在后面句子中有不同的含义。
接下来，通过使用词汇和语法规则，理解每个单词的含义。
然而，有些词有类似的含义（同义词），有些词有多重含义（多义词）。

### 自然语言生成(NLG)
NLG 是从结构化数据中以可读地方式自动生成文本的过程。自然语言生成的问题是难以处理。
自然语言生成可被分为三个阶段：
1. 文本规划：完成结构化数据中基础内容的规划。
2. 语句规划：从结构化数据中组合语句，来表达信息流。
3. 实现：产生语法通顺的语句来表达文本。
