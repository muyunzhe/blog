---
title: 随机森林跟GDBT的区别
comments: true
toc: true
mathjax2: true
categories: [artificial-intelligence,machine-learning]
tags: [machine-learning,classification]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
date: 2017-05-09 19:45:26
---
决策树这种算法有着很多良好的特性，比如说训练时间复杂度较低，预测的过程比较快速，模型容易展示。单决策树又有一些不好的地方，比如说容易over-fitting。多个决策树协同决策，能够有效提升模型的预测精度。
 <!--more-->
## bagging
1. 基于bootstrap sampling 自助采样法，重复性有放回的随机采用部分样本进行训练最后再将结果 voting 或者 averaging 。
2. 并行式算法
3. 每一层大约花费时间相同
4. 主要关注于降低方差，但是不能降低偏差

## 随机森林
在bagging的基础上，加入随机属性选择机制。
1. 对于回归模型，选择属性数量，建议选择全部属性的三分之一
2. 对于分类模型，选择属性数量，建议选择全部属性的平方根
3. 随机森林对于高维数据集的处理能力很好，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。
4. 高度并行化，易于分布式实现
5. 在解决回归问题时并没有它在分类中变现那么好，主要因为它不能给出一个连续的输出
6. 无法控制模型内部的运行，等同于黑盒子
7. 调参方法：网格搜索
8. n_estimators越多越好，但计算量对大增
9. 随机森林不进行剪枝。决策树剪枝是因为防止过拟合，而随机森林的“随机”已经防止了过拟合，因此不需要剪枝。
## GBDT
GBDT中的树都是回归树，不是分类树 ，因为gradient boost 需要按照损失函数的梯度近似的拟合残差，这样拟合的是连续数值，因此只有回归树。
在Gradient Boosting中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boosting对正确、错误样本进行加权有着很大的区别。

## XGBoost
单独再说吧

## 随机森林与GBDT区别
相同点：
（1）都是由多棵树组成的，都是集成学习算法
（2）最终的结果都是由多颗树一起决定

不同点：
(1)组成随机森林的树可以是分类树，也可以是回归树，但是GBDT只能由回归树组成。
(2)组成随机森林的树可以并行生成，但是组成GBDT的树只能串行生成。
(3)对于最终的输出结果，随机森林采用多数投票；而GBDT是将所有的结果累加起来，或者加权起来
(4)随机森林对异常值不敏感，而GBDT对异常值非常敏感
(5)随机森林通过减小方差来提高性能，GBDT通过减小偏差来提高性能
