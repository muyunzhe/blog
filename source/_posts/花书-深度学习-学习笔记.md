---
title: 花书(深度学习)学习笔记
comments: true
toc: true
mathjax2: true
date: 2017-10-09 11:23:24
categories: [artificial-intelligence,deep-learning]
tags: [deep-learning,Neural Networks]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
---
带着问题学习花书(深度学习)
 <!--more-->
 1.列举常见的一些范数及其应用场景，如L0，L1，L2，L∞，Frobenius范数
 向量范数：
 −∞ -范数：所有向量元素中绝对值的最小值；
 0-范数：向量中非零元素的数量；
 1-范数：向量元素绝对值之和，也称街区距离（city-block）；
 2-范数：向量元素的平方和再开方。Euclid范数，也称欧几里得范数，欧氏距离；
 ∞ -范数：所有向量元素中绝对值的最大值，也称棋盘距离（chessboard），切比雪夫距离
 矩阵范数：
 1-范数：列和范数，即所有矩阵列向量绝对值之和的最大值
 ∞-范数：行和范数，即所有矩阵行向量绝对值之和的最大值
 2-范数：p=2且m=n方阵时，称为谱范数。矩阵A的谱范数是 A 最大的奇异值或半正定矩阵的最大特征值的平方根
 F-范数：Frobenius范数（希尔伯特-施密特范数，这个称呼只在希尔伯特空间），即矩阵元素绝对值的平方和再开平方
 核范数：若 A 矩阵是方阵，称为本征值。若不是方阵，称为奇异值，即奇异值/本征值之和

2.随机梯度下降m的选择
太大太小都不行，最好是64-1024之间的，符合内存存储原理的2的n次方的一个数

3.指数加权平均
Yn = a*Y(n-1) + (1-a) * Xn
这个a是个系数，决定了之前的数据在当天的结果中所占的比重。
具体是到之前多少天呢？就是a的n次方的值降低到1/e=0.35以下，这个n就是这个天数。

这个指数加权平均有个缺点就是：启动慢，如果Y0初始化为0，那么Y1就特别低。怎么解决呢？
让Yt用Yt/(1-a^t)代替。这就叫偏差修正。

如何应用到动量梯度下降(Momentum)上？
参考之前几次的dw、db，可以在摆时相互抵消。通常不用偏差修正，因为偏差修正在迭代几次之后就没用了。

RMSprop是参考了dw\db的二阶导数。

4.超参数的选择
两个原则：
随机
从粗糙到精密
有效范围：
0.0001 0.001 0.1 1之间的随机




5.

 2.简单介绍一下贝叶斯概率与频率派概率，以及在统计中对于真实参数的假设。3.概率密度的万能近似器4.简单介绍一下sigmoid，relu，softplus，tanh，RBF及其应用场景5.Jacobian，Hessian矩阵及其在深度学习中的重要性6.KL散度在信息论中度量的是那个直观量7.数值计算中的计算上溢与下溢问题，如softmax中的处理方式8.与矩阵的特征值相关联的条件数(病态条件)指什么，与梯度爆炸与梯度弥散的关系9.在基于梯度的优化问题中，如何判断一个梯度为0的零界点为局部极大值／全局极小值还是鞍点，Hessian矩阵的条件数与梯度下降法的关系10.KTT方法与约束优化问题，活跃约束的定义11.模型容量，表示容量，有效容量，最优容量概念12.正则化中的权重衰减与加入先验知识在某些条件下的等价性13.高斯分布的广泛应用的缘由14.最大似然估计中最小化KL散度与最小化分布之间的交叉熵的关系15.在线性回归问题，具有高斯先验权重的MAP贝叶斯推断与权重衰减的关系，与正则化的关系16.稀疏表示，低维表示，独立表示17.列举一些无法基于地图的优化来最小化的代价函数及其具有的特点18.在深度神经网络中，引入了隐藏层，放弃了训练问题的凸性，其意义何在19.函数在某个区间的饱和与平滑性对基于梯度的学习的影响20.梯度爆炸的一些解决办法21.MLP的万能近似性质22.在前馈网络中，深度与宽度的关系及表示能力的差异23.为什么交叉熵损失可以提高具有sigmoid和softmax输出的模型的性能，而使用均方误差损失则会存在很多问题。分段线性隐藏层代替sigmoid的利弊24.表示学习的发展的初衷？并介绍其典型例子:自编码器25.在做正则化过程中，为什么只对权重做正则惩罚，而不对偏置做权重惩罚26.在深度学习神经网络中，所有的层中考虑使用相同的权重衰减的利弊27.正则化过程中，权重衰减与Hessian矩阵中特征值的一些关系，以及与梯度弥散，梯度爆炸的关系28.L1／L2正则化与高斯先验／对数先验的MAP贝叶斯推断的关系29.什么是欠约束，为什么大多数的正则化可以使欠约束下的欠定问题在迭代过程中收敛30.为什么考虑在模型训练时对输入(隐藏单元／权重)添加方差较小的噪声，与正则化的关系31.共享参数的概念及在深度学习中的广泛影响32.Dropout与Bagging集成方法的关系，以及Dropout带来的意义与其强大的原因33.批量梯度下降法更新过程中，批量的大小与各种更新的稳定性关系34.如何避免深度学习中的病态，鞍点，梯度爆炸，梯度弥散35.SGD以及学习率的选择方法，带动量的SGD对于Hessian矩阵病态条件及随机梯度方差的影响
