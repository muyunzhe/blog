---
title: 深度学习之简要介绍
comments: true
toc: true
mathjax2: true
categories: [artificial-intelligence,deep-learning]
tags: [deep-learning,Neural Networks]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
date: 2017-01-19 11:03:10
---
神经网络是深度学习的基础，感知器是最简单的神经网络结构，因此关于深度学习的篇章从感知器开始学起。
 <!--more-->
本文转载自机器之心，原文来自KDnugget，作者：Ujjwal Karn，链接：http://www.jiqizhixin.com/article/1886
## 神经元
神经网络中计算的基本单元是神经元，一般称作「节点」（node）或者「单元」（unit）。节点从其他节点接收输入，或者从外部源接收输入，然后计算输出。每个输入都辅有「权重」（weight，即 w），权重取决于其他输入的相对重要性。节点将函数 f（定义如下）应用到加权后的输入总和，如图 1 所示：
![](/img/716-600x350.jpg)
神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。

## 单层神经网络(感知器)
在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。

我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。

![单个神经元](/img/单个神经元.png)
此网络接受 X1 和 X2 的数值输入，其权重分别为 w1 和 w2。另外，还有配有权重 b（称为「偏置（bias）」）的输入 1。

函数f是激活函数，在实践中常用的激活函数包括：
![不同的激活函数](/img/不同的激活函数.png)

偏置bias的作用：
then the output of the network becomes sig(w0*x + w1*1.0),x 为输入，w0是输入的权重，1.0是偏置，w1是偏置的权重，则结果如下：
![偏置的作用](/img/偏置的作用.png)

## 两层神经网络（多层感知器）
多层感知器（Multi Layer Perceptron，即 MLP）包括至少一个隐藏层（除了一个输入层和一个输出层以外）。单层感知器只能学习线性函数，而多层感知器也可以学习非线性函数。
![有一个隐藏层的多层感知器](/img/有一个隐藏层的多层感知器.png)

需要说明的是，在两层神经网络中，我们不再使用sgn函数作为函数g，而是使用平滑函数sigmoid作为函数g。我们把函数g也称作激活函数（active function）。

事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。

## 多层神经网络（深度学习）
在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。
![](/img/331-802x629.jpg)
多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做“正向传播”。

增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。

# 总结
![](/img/40-880x584.jpg)

可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。

神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。

## 前馈神经网络
具体到前馈神经网络中，就有了本文中所分别描述的三个网络：单层神经网络，双层神经网络，以及多层神经网络。其结构都大致如下：
![一个前馈神经网络的例子](/img/一个前馈神经网络的例子.png)
一个前馈神经网络可以包含三种节点：
1. 输入节点（Input Nodes）：输入节点从外部世界提供信息，总称为「输入层」。在输入节点中，不进行任何的计算——仅向隐藏节点传递信息。
2. 隐藏节点（Hidden Nodes）：隐藏节点和外部世界没有直接联系（由此得名）。这些节点进行计算，并将信息从输入节点传递到输出节点。隐藏节点总称为「隐藏层」。尽管一个前馈神经网络只有一个输入层和一个输出层，但网络里可以没有也可以有多个隐藏层。
3. 输出节点（Output Nodes）：输出节点总称为「输出层」，负责计算，并从网络向外部世界传递信息。

在前馈网络中，信息只单向移动——从输入层开始前向移动，然后通过隐藏层（如果有的话），再到输出层。在网络中没有循环或回路 [3]（前馈神经网络的这个属性和递归神经网络不同，后者的节点连接构成循环）

两个前馈神经网络的例子：单层感知器——这是最简单的前馈神经网络，不包含任何隐藏层。多层感知器——多层感知器有至少一个隐藏层。

## 反向传播(BP)
最初，所有的边权重（edge weight）都是随机分配的。对于所有训练数据集中的输入，人工神经网络都被激活，并且观察其输出。这些输出会和我们已知的、期望的输出进行比较，误差会「传播」回上一层。该误差会被标注，权重也会被相应的「调整」。该流程重复，直到输出误差低于制定的标准。

### 为什么要反向传播
神经网络模型的学习算法一般是SGD。SGD需要用到损失函数C关于各个权重参数的偏导数。一个模型的参数w,b是非常多的，故而需要反向传播算法快速计算。也就是说反向传播算法是一种计算偏导数的方法。
有两种求导模式：前向传播和反向传播
前向传播计算太复杂，因此通常采用后向传播

## 存在的问题
1. 它是一个全连接的网络，因此在输入比较大的时候，权值会特别多
2. 梯度发散
