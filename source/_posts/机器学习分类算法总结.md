---
title: 机器学习分类算法总结
comments: true
categories: [artificial-intelligence,machine-learning]
tags: [machine-learning,classification]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
date: 2016-12-23 16:01:51
---
主要通过模型、策略、算法、特点、优缺点等角度对分类算法进行学习总结
 <!--more-->
# 分类
## knn（多类分类，判别模型）
KNN(K-Nearest Neighbor)
描述：给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的k个训练实例，然后统计最近的k个训练实例中所属类别计数最多的那个类，就是新实例的类
特点：特种空间、样本点
### 三要素：
1. k值的选择
k值越小表明模型越复杂，更加容易过拟合
但是k值越大，模型越简单，如果k=N的时候就表明无论什么点都是训练集中类别最多的那个类
2. 距离的度量（常见的距离度量有欧式距离，马氏距离等）
3. 分类决策规则 （多数表决规则）

### 优缺点：
优点：
思想简单，理论成熟，既可以用来做分类也可以用来做回归；
可用于非线性分类；
训练时间复杂度为O(n)；
准确度高，对数据没有假设，对outlier不敏感；
缺点：
计算量大；
样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；
需要大量的内存；

## 朴素贝叶斯（多类分类，生成模型）
描述：P(A|B)=P(A∩B)P(B) 对于给出的待分类项，求解在此项出现的条件下各个目标类别出现的概率，哪个最大，就认为此待分类项属于哪个类别
特点：独立假设
学习策略：极大似然估计、极大后验概率估计
损失函数：对数似然函数
学习方法：概率计算公式、EM算法
优点：
特征为离散值时直接统计即可（表示统计概率）
对小规模的数据表现很好，适合多分类任务，适合增量式训练。
缺点：
对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。
### 贝叶斯网络


## 决策树（多类分类，判别模型）
描述：根据信息熵的大小选择一个属性进行分枝
特点：分类树、回归树
学习策略：正则化的极大似然估计
损失函数：对数似然损失
学习算法：特征选择、生成、剪枝
### id3
信息增益

### C4.5
信息增益比

### CART
基尼指数

### 连续与缺失值

### 多变量决策树

容易过拟合（随机森林可以减小过拟合）
解决：在目标函数中加入正则化项，增大数据量，减少模型复杂程度，决策树剪枝，神经网络Dropout策略，观察训练和交叉验证的loss图像


## 逻辑斯蒂回归（多类分类，判别模型）
学习策略：极大似然估计、正则化的极大似然估计
损失函数：逻辑斯蒂损失
学习算法：迭代尺度算法、梯度下降、拟牛顿算法



## SVM（二类分类，判别模型）
描述：svm的基本想法就是求解能正确划分训练样本并且其几何间隔最大化的超平面。
特点：分离超平面，核技巧
学习策略：极小化正则化合页损失、软间隔最大化
损失函数：合页损失
学习算法：序列最小最优化算法（SMO）

### 为什么要引入对偶算法
对偶问题往往更加容易求解(结合拉格朗日和kkt条件)
可以很自然的引用核函数（拉格朗日表达式里面有内积，而核函数也是通过内积进行映射的）
### 核函数
将输入特征x（线性不可分）映射到高维特征R空间，可以在R空间上让SVM进行线性可以变，这就是核函数的作用
多项式核函数:K(x,z)=(x∗z+1)pK(x,z)=(x∗z+1)p
高斯核函数:K(x,z)=exp(−(x−z)2σ2)K(x,z)=exp(−(x−z)2σ2)
字符串核函数：貌似用于字符串处理等
### SVM优缺点
优点：
使用核函数可以向高维空间进行映射
使用核函数可以解决非线性的分类
分类思想很简单，就是将样本与决策面的间隔最大化
分类效果较好
缺点：
对大规模数据训练比较困难
无法直接支持多分类，但是可以使用间接的方法来做

### 对于多分类
1. 直接法
直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该优化就可以实现多分类（计算复杂度很高，实现起来较为困难）

2. 间接法
一对多
其中某个类为一类，其余n-1个类为另一个类，比如A,B,C,D四个类，第一次A为一个类，{B,C,D}为一个类训练一个分类器，第二次B为一个类,{A,C,D}为另一个类,按这方式共需要训练4个分类器，最后在测试的时候将测试样本经过这4个分类器f1(x)f1(x),f2(x)f2(x),f3(x)f3(x)和f4(x)f4(x),取其最大值为分类器(这种方式由于是1对M分类，会存在偏置，很不实用)
一对一(libsvm实现的方式)
任意两个类都训练一个分类器，那么n个类就需要n*(n-1)/2个svm分类器。
还是以A,B,C,D为例,那么需要{A,B},{A,C},{A,D},{B,C},{B,D},{C,D}为目标共6个分类器，然后在预测的将测试样本通过这6个分类器之后进行投票选择最终结果。（这种方法虽好，但是需要n*(n-1)/2个分类器代价太大，不过有好像使用循环图来进行改进）

### 多分类不平衡问题

### 支持向量回归

## SVM、LR、决策树的对比？


## 提升方法（二类分类，判别模型）
特点：弱分类器的线性组合
学习策略：极小化加法模型的指数损失
损失函数：指数损失
学习算法：前向分布加法算法

### Bootstrap(自助法)
一种抽样方法

### Bagging（代表为随机森林）
描述：bagging的思想是从训练集中进行子抽样，对抽取的各子训练集建立多个基模型，对所有基模型预测的结果进行综合产生最终的预测结果
随机森林的随机包含两部分内容：
1. 训练样本选择方面的Random：
Bootstrap方法随机选择子样本
2. 特征选择方面的Random：
属性集中随机选择k个属性，每个树节点分裂时，从这随机的k个属性，选择最优的(如何选择最优又有各种最大增益的方法，不在本文讨论范围内)。

>从样本集中用Bootstrap采样选出n个训练样本(放回，因为别的分类器抽训练样本的时候也要用)
在所有属性上，用这n个样本训练分类器（CART or SVM or ...）
重复以上两步m次，就可以得到m个分类器（CART or SVM or ...）
将数据放在这m个分类器上跑，最后投票机制(多数服从少数)看到底分到哪一类(分类问题)

优点：
1.不容易出现过拟合，因为选择训练样本的时候就不是全部样本。
2.可以既可以处理属性为离散值的量，比如ID3算法来构造树，也可以处理属性为连续值的量，比如C4.5算法来构造树。
3.对于高维数据集的处理能力令人兴奋，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。此外，该模型能够输出变量的重要性程度，这是一个非常便利的功能。
4.分类不平衡的情况时，随机森林能够提供平衡数据集误差的有效方法
5.两个随机性的引入，使得随机森林具有很好的抗噪声能力
6.训练速度快，可以得到变量重要性排序。
7.在训练过程中，能够检测到feature间的互相影响。
缺点
1.随机森林在解决回归问题时并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续型的输出。当进行回归时，随机森林不能够作出超越训练集数据范围的预测，这可能导致在对某些还有特定噪声的数据进行建模时出现过度拟合。
2.对于许多统计建模者来说，随机森林给人的感觉像是一个黑盒子——你几乎无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试。

### Boosting（代表为GBDT）
GBDT，全称(Gradient Boosting Decision Tree)
描述：一种迭代算法，针对同一个训练集训练不同的分类器(弱分类器)，然后进行分类，对于分类正确的样本权值低，分类错误的样本权值高（通常是边界附近的样本），最后的分类器是很多弱分类器的线性叠加（加权组合），分类器相当简单。实际上就是一个简单的弱分类算法提升(boost)的过程。

优点
1.可以使用各种方法构造子分类器，Adaboost算法提供的是框架
2.简单，不用做特征筛选
3.相比较于RF，更不用担心过拟合问题
缺点
1.对于噪音数据和异常数据是十分敏感的，因此在每次迭代时候会给噪声点较大的权重。
2.运行速度慢，凡是涉及迭代的基本上都无法采用并行计算，Adaboost是一种"串行"算法.所以GBDT(Gradient Boosting Decision Tree)也非常慢。

### 多样性

### GBDT 和 RF 的区别？
1.RF并行  GBDT串行
2.GBDT的基模型为弱模型，而RF中的基树是强模型
3.GBDT重采样的不是样本，而是样本的分布，每次迭代之后，样本的分布会发生变化，也就是被分错的样本会更多的出现在下一次训练集中
4.bagging是减少variance，而boosting是减少bias
5.对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来
6.随机森林对异常值不敏感，GBDT对异常值非常敏感；
7.随机森林对训练数据一视同仁，GBDT是基于权值的弱分类器的集成


## 神经网络
### 感知机与多层网络
。。。。。。
