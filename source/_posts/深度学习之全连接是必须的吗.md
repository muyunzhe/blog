---
title: 深度学习之全连接是必须的吗
comments: true
toc: true
mathjax2: true
date: 2017-12-12 14:45:49
categories: [artificial-intelligence,deep-learning]
tags: [deep-learning,Neural Networks]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
---
为什么好像所有的卷积层之后都要加上全连接层？全连接层的作用是什么？如果说神经网络是模仿人的大脑，可是人的大脑不是好像没有全连接层的吗？
 <!--more-->
## 全连接层的作用：
 全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽。
 目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能。
 在FC越来越不被看好的当下，我们近期的研究（In Defense of Fully Connected Layers in Visual Representation Transfer）发现，FC可在模型表示能力迁移过程中充当“防火墙”的作用。具体来讲，假设在ImageNet上预训练得到的模型为 ，则ImageNet可视为源域（迁移学习中的source domain）。微调（fine tuning）是深度学习领域最常用的迁移学习技术。针对微调，若目标域（target domain）中的图像与源域中图像差异巨大（如相比ImageNet，目标域图像不是物体为中心的图像，而是风景照，见下图），不含FC的网络微调后的结果要差于含FC的网络。因此FC可视作模型表示能力的“防火墙”，特别是在源域与目标域差异较大的情况下，FC可保持较大的模型capacity从而保证模型表示能力的迁移。（冗余的参数并不一无是处。）

## 全连接层的缺点
1. 参数太多
2. 容易过拟合
3. 不可解释

实验证明，将其替换为平均池化层（或者1x1卷积层）不仅不影响精度，还可以减少参数量。
