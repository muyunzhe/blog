---
title: 机器学习回归算法总结
comments: true
categories: [artificial-intelligence,machine-learning]
tags: [machine-learning,regression]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
date: 2016-12-29 14:28:08
---
主要通过模型、策略、算法、特点、优缺点等角度对回归算法进行学习总结
 <!--more-->
 # 回归
 回归其实就是对已知公式的未知参数进行估计。比如已知公式是y = a*x + b，未知参数是a和b。我们现在有很多真实的(x,y)数据（训练样本），回归就是利用这些数据对a和b的取值去自动估计。
 回归的前提是公式已知，否则回归无法进行。
 根据这些公式的不同，回归分为线性回归和非线性回归。线性回归中公式都是“一次”的（一元一次方程，二元一次方程...），而非线性则可以有各种形式（N元N次方程，log方程 等等）。

 ## Kmeans/knn
 knn也可用于回归，在找到最近的k个实例之后，可以计算这k个实例的平均值作为预测值。或者还可以给这k个实例添加一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）。


 ## 决策树
 CART － Classification and Regression Trees
 分类与回归树，是二叉树，可以用于分类，也可以用于回归问题，最先由 Breiman 等提出。
 分类树的输出是样本的类别， 回归树的输出是一个实数。
 ![回归树算法](/img/回归树算法.png)

 ## 线性回归

 ## 逻辑回归
 很多情况下，我们需要回归产生一个类似概率值的0~1之间的数值，于是引入了Logistic方程，来做归一化。
 所以，Logistic Regression 就是一个被logistic方程归一化后的线性回归，仅此而已。
 1. 可用于概率预测，也可用于分类。
 2. 仅能用于线性问题
 3. 分类时计算量非常小，速度很快，存储资源低；
 4. 各feature之间不需要满足条件独立假设，但各个feature的贡献是独立计算的。逻辑回归不像朴素贝叶斯一样需要满足条件独立假设（因为它没有求后验概率）。但每个feature的贡献是独立计算的，即LR是不会自动帮你combine 不同的features产生新feature的
 5. 只能处理二分类问题

 ### 梯度下降
 会陷入局部最优，并且每次在对当前样本计算cost的时候都需要去遍历全部样本才能得到cost值，这样计算速度就会慢很多（虽然在计算的时候可以转为矩阵乘法去更新整个w值）
 所以现在好多框架（mahout）中一般使用随机梯度下降法，它在计算cost的时候只计算当前的代价，最终cost是在全部样本迭代一遍之求和得出，还有他在更新当前的参数w的时候并不是依次遍历样本，而是从所有的样本中随机选择一条进行计算，它方法收敛速度快（一般是使用最大迭代次数），并且还可以避免局部最优，并且还很容易并行（使用参数服务器的方式进行并行）

 ### 局部加权线性回归

 ### 岭回归

 ### 前向逐步回归
