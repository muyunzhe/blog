---
title: EM算法(最大期望算法)
comments: true
categories: [artificial-intelligence,algorithm]
tags: [algorithm, EM]
keywords: 'Data Mining,EM'
date: 2016-10-09 16:04:51
---
EM算法是一种迭代算法，用于含有隐含变量（hidden variable）的概率模型参数的极大似然估计，或极大后验概率估计。
EM算法的每次迭代由两步组成：E步，求期望（expectation）; M步，求极大（maximization）
 <!--more-->
# 相关统计知识
## 极大似然估计
极大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。
最大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。
求最大似然函数估计值的一般步骤：
（1）写出似然函数；
（2）对似然函数取对数，并整理；
（3）求导数，令导数为0，得到似然方程；
（4）解似然方程，得到的参数即为所求；

## Jensen不等式
如果f是上凸函数，X是随机变量，那么f(E[X]) ≥ E[f(X)]
>上凸函数：函数f(x)满足对定义域上任意两个数a,b都有f[(a+b)/2] ≥ [f(a)+f(b)]/2

特别地，如果f是严格上凸函数，那么E[f(X)] = f(E[X])当且仅当p(X=E[X])，也就是说X是常量。
!["图片描述"](/img/201104061615564890.png)

# EM算法的引入
也就是为了解决什么问题：如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐含变量时，就不能简单的使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法。

EM算法首先选取参数的初值，只有通过迭代计算参数的估计值，直到收敛为知。
那么问题来了：
1. 初值怎么选
2. 怎么个迭代法
2. 怎么算收敛

## 初值怎么选
EM算法与初值的选择有关，选择不同的初值可能得到不同的参数估计值
通常情况下，选取几个不同的初值进行迭代，然后对得到的各个估计值加以比较，从中选择最好的
## 迭代法
E步：

## 收敛性
参数估计序列只能收敛到对数似然函数序列的稳定点，不能保证收敛到极大值点


# EM算法的收敛性
# EM算法的应用
# EM算法的推广
