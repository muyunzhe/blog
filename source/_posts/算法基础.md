---
title: 算法基础
comments: true
categories: Algorithm
tags: Data Mining
keywords: 'Hexo, Blog'
date: 2016-10-17 16:10:17
---
机器学习之路初级：统计学基础知识
 <!--more-->
# 线性回归
线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。

# 梯度下降算法
梯度下降法(Gradient Descent, GD)是一个最优化算法，通常也称为最速下降法。最速下降法是用负梯度方向为搜索方向的，最速下降法越接近目标值，步长越小，前进越慢。梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）。
思想：从当前点出发，找最陡的坡走一步，到达新的点之后，再找最陡的坡，这样一步一步，快速走到最低点（最小损失函数的收敛点）

## 优点
只需求解损失函数的一阶导数，计算的代价比较小

# 梯度下降算法与随机梯度下降算法
## 梯度下降算法：
需要把m个样本全部带入计算，迭代一次计算量为m*n^2
可以得到全局最优解，易于并行实现
样本数目很多时，训练过程很慢

## 随机梯度下降算法：
每次只使用一个样本，迭代一次计算量为n^2，当m很大的时候，随机梯度下降迭代一次的速度要远高于梯度下降
训练速度快
得到的是局部最优解，不易于并行实现

# 最小二乘法
普通最小二乘法（ Ordinary  Least Square，OLS）：所选择的回归模型应该使所有观察值的残差平方和达到最小。
从Cost/Loss function角度去想，这是统计（机器）学习里面一个重要概念，一般建立模型就是让loss function最小，而最小二乘法可以认为是 loss function = （y_hat -y )^2的一个特例

# 最小二乘法与梯度下降算法
如果把最小二乘看做是优化问题的话，那么梯度下降是求解方法的一种（线性和非线性都可以），x=(A^T A)^{-1}A^Tb是求解线性最小二乘的一种，高斯-牛顿法和Levenberg-Marquardt则能用于求解非线性最小二乘。
知乎里有篇帖子分析的不错，[最小二乘法和梯度下降法有哪些区别？](http://www.zhihu.com/question/20822481)

# 牛顿法
使用迭代的方法 寻找使f( θ )=0 的 θ 值
先随机选一个点，然后求出f在该点的切线，即f在该点的导数。该切线等于0的点，即该切线与x轴相交的点为下一次迭代的值。直至逼近f等于0的点。过程如下图：
![](/img/nA36bau.png)

牛顿方法通常比梯度下降收敛速度快，迭代次数也少。
但因为要计算Hessian矩阵的逆，所以每次迭代计算量比较大。当Hessian矩阵不是很大时牛顿方法要优于梯度下降。

# 拟牛顿法
牛顿法需要计算Hessian矩阵的逆矩阵，运算复杂度太高。因此，很多牛顿算法的变形出现了，这类变形统称拟牛顿算法。拟牛顿算法的核心思想用一个近似矩阵\(B\)替代逆Hessian矩阵\({H^{ - 1}}\)。不同算法的矩阵\(B\)的计算有差异，但大多算法都是采用迭代更新的思想在tranning的每一轮更新矩阵\(B\)。
