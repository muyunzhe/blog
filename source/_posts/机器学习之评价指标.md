---
title: 机器学习之评价指标
comments: true
toc: true
mathjax2: true
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
date: 2017-03-03 11:09:40
categories: [artificial-intelligence,machine-learning]
tags: [machine-learning,AUC]
---
很多项目中都是用AUC来评价分类器的好坏，而不是使用精确率，召回率，F1值，请问这是什么原因呢？他们各自有什么优缺点和使用场景啊？
 <!--more-->
## 精确度与召回率
精确率是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)，也就是P=TP/(TP+FP)

召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。也就是P=TP/(TP+FN)。

其实就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。

另外还有一个准确率，即预测对的/所有=（TP + TN）/（TP+FN+TN+FP）

## F1值
 F值  = 正确率 * 召回率 * 2 / (正确率 + 召回率) （F 值即为正确率和召回率的调和平均值）

## ROC
ROC（receiver operating characteristic curve）是曲线。一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting（比如图中0.2到0.4可能就有问题，但是样本太少了），这个时候调模型可以只看AUC，面积越大一般认为模型越好。
ROC 关注两个指标，true positive rate:TP/(TP+FN)和false positive rate:FP/(FP+TN)
![](/img/roc.jpg)
直观上，TPR 代表能将正例分对的概率，FPR 代表将负例错分为正例的概率。在 ROC 空间中，每个点的横坐标是 FPR，纵坐标是 TPR，这也就描绘了分类器在 TP（真正率）和 FP（假正率）间的 trade-off。

## PRC
PRC， precision recall curve。和ROC一样，先看平滑不平滑（蓝线明显好些），在看谁上谁下（同一测试集上），一般来说，上面的比下面的好（绿线比红线好）。F1（计算公式略）当P和R接近就也越大，一般会画连接(0,0)和(1,1)的线，线和PRC重合的地方的F1是这条线最大的F1（光滑的情况下），此时的F1对于PRC就好象AUC对于ROC一样。一个数字比一条线更方便调模型。
![](/img/rpc.jpg)

## AUC
AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。
AUC值越大的分类器，正确率越高.
AUC=1AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。
0.5<AUC<10.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
AUC=0.5AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
AUC<0.5AUC<0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在 AUC<0.5AUC<0.5 的情况。

## 总结
既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。而PR则会出现大变化。

ROC曲线和PR曲线的关系
在ROC空间，ROC曲线越凸向左上方向效果越好。与ROC曲线左上凸不同的是，PR曲线是右上凸效果越好。

PR曲线会面临一个问题，当需要获得更高recall时，model需要输出更多的样本，precision可能会伴随出现下降/不变/升高，得到的曲线会出现浮动差异（出现锯齿），无法像ROC一样保证单调性。

PRC相对的优势
当正负样本差距不大的情况下，ROC和PR的趋势是差不多的，但是当负样本很多的时候，两者就截然不同了，ROC效果依然看似很好，但是PR上反映效果一般。解释起来也简单，假设就1个正例，100个负例，那么基本上TPR可能一直维持在100左右，然后突然降到0.如图，(a)(b)分别为正负样本1:1时的ROC曲线和PR曲线，二者比较接近。而(c)(d)的正负样本比例为1:1，这时ROC曲线效果依然很好，但是PR曲线则表现的比较差。这就说明PR曲线在正负样本比例悬殊较大时更能反映分类的性能。
在正负样本分布得极不均匀(highly skewed datasets)的情况下，PRC比ROC能更有效地反应分类器的好坏。

学术论文在假定正负样本均衡的时候多用ROC/AUC，实际工程更多存在数据标签倾斜问题一般使用F1。

参考文章：
* [机器学习性能评估指标](http://charleshm.github.io/2016/03/Model-Performance/#)
* [机器学习评价指标大汇总](http://www.zhaokv.com/2016/03/ml-metric.html)
