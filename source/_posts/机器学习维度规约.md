---
title: 机器学习维度规约
comments: true
toc: true
mathjax2: true
categories: [artificial-intelligence,machine-learning]
tags: [machine-learning,PCA, LDA]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
date: 2017-01-10 20:31:17
---
任何分类或者回归方法的复杂度都依赖于样本的规模N和输入的维度d，而特征选择和特征提取是较少复杂性的有效手段。也就是所谓的维度规约
 <!--more-->
## 特征选择
从d个维度中找出提供最多信息的k个维度

### 子集选择
d个变量有2的d次方个可能的子集，在如此多可能情况下如何选择最佳子集呢？

用F代表输入维的特征xi的集合，E（F）表示当只使用F中的输入时，在验证样本上出现的误差。
#### 向前选择
思想：从空集开始，逐渐添加变量，每次添加一个使得误差降低最多的变量，直到进一步的添加不会降低误差或降低很少
算法：从F=空开始，在每一步，对于所有可能的输入xi，训练我们的模型并在验证集上计算E（F∪xi），然后我们选择导致最小误差的输入xi，并将xi加入到F。也可以设定一个阈值，提前结束算法。

#### 向后选择
思想：从所有变量集合中，逐个删除变量，每一步删除降低误差最多的变量，直到进一步的删除不会降低误差或者降低很少
算法：同上相反

这两种情况下，误差检测都应该在不同于训练集的验证集上进行，因为我们需要检测泛化准确率，使用更多的特征一般会有更低的训练误差，但不一定有更低的验证误差。
开销很大，且不能保证找到最佳子集（因为xi和xj本身可能效果不好，但是合起来可能误差降低很多），是个局部最优化算法。可以通过一次增加多个甚至还可以去掉之前增加的特征的方法提高找到最佳子集的可能性，但也增加可复杂度
在无用特征大于有用特征时，选择向前搜索，反之选择向后搜索

人脸识别这样的应用中，特征选择不是降维的好方法，携带脸部识别信息的是许多像素值的组合，因此可以用特征提取方法来实现。

## 特征提取
找出k个维的新集合，这些维是原来d个维的组合，最主要的方法包括主成分分析(PCA)和线性判别分析(LDA)，分别为非监督的和监督的。

### 主成分分析（非监督方法）
Principal Component Analysis(PCA)是最常用的线性降维方法，它的目标是通过某种线性投影，将高维的数据映射到低维的空间中表示，并期望在所投影的维度上数据的方差最大，以此使用较少的数据维度，同时保留住较多的原数据点的特性。
需要最大化的准则是方差
主成分是这样的wi，样本投影到wi上之后最分散，使得样本点之间的差别变得最明显。为了得到唯一解且使该方向成为最重要因素，我们要求||wi||=1.
也就是输入样本的协方差矩阵的最大特征值对应的特征向量
对离群点很敏感
适用于d<N的情况
主成分分析是通过变量变换把注意力集中在具有较大变差的那些主成分上，而舍弃那些变差小的主成分；
用原始变量的线性组合表示新的综合变量，即主成分；

PCA不能被用来作为防止过拟合的手段，要防止过拟合，最好还是采用规则化。因为PCA不考虑label信息。
因此只有在内存不足、数据量太大导致硬盘不足等情况下才考虑PCA，否则，不建议滥用PCA。

#### 特征嵌入
![特征嵌入](/img/特征嵌入.jpg)
适用于d>N的情况

#### 因子分析
把注意力集中在少数不可观测的潜在变量（即公共因子）上，而舍弃特殊因子。
因子分析就是认为高维样本点实际上是由低维样本点经过高斯分布、线性变换、误差扰动生成的。
因子分析通过研究众多变量之间的内部依赖关系，探求观测数据的基本结构。并用少数几个假想变量来 表示其基本结构。这种假想变量能够反应原来众多变量的主要信息，属于不可观测的潜在变量，称为银子。
潜在的假想变量和随机影响变量的线性组合表示原始变量。
因子分析有两个核心问题：一是如何构造因子变量，二是如何对因子变量进行命名解释。因子分析有下面4个基本步骤：
1. 确定原有若干变量是否适合于因子分析。因子分析的基本逻辑是从原始变量中构造出少数几个具有代表意义的因子变量，这就要求原有变量之间要具有比较强的相关性，否则，因子分析将无法提取变量间的“共性特征”（变量间没有共性还如何提取共性？）。实际应用时，可以使用相关性矩阵进行验证，如果相关系数小于0.3，那么变量间的共性较小，不适合使用因子分析。
2. 构造因子变量。因子分析中有多种确定因子变量的方法，如基于主成分模型的主成分分析法和基于因子分析模型的主轴因子法、极大似然法、最小二乘法等。其中基于主成分模型的主成分分析法是使用最多的因子分析方法之一。
3. 利用旋转使得因子变量更具有可解释性。在实际分析工作中，主要是因子分析得到因子和原变量的关系，从而对新的因子能够进行命名和解释，否则其不具有可解释性的前提下对比PCA就没有明显的可解释价值。
4. 计算因子变量的得分。计算因子得分是因子分析的最后一步，因子变量确定以后，对每一样本数据，希望得到它们在不同因子上的具体数据值，这些数值就是因子得分，它和原变量的得分相对应。


### 线性判别分析(监督方法)
线性判别分析(linear discriminant analysis, LDA)是一种用于分类问题的维度规约的监督方法。
分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。
局限性：
1. 当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出
2. LDA不适合对非高斯分布的样本进行降维
3. LDA在样本分类信息依赖方差而不是均值时，效果不好
4. LDA可能过度拟合数据

### PCA和LDA的区别
![PCA和LDA的区别](/img/PCA和LDA的区别.png)
上图左侧是PCA的降维思想，它所作的只是将整组数据整体映射到最方便表示这组数据的坐标轴上，映射时没有利用任何数据内部的分类信息。因此，虽然PCA后的数据在表示上更加方便（降低了维数并能最大限度的保持原有信息），但在分类上也许会变得更加困难；上图右侧是LDA的降维思想，可以看到LDA充分利用了数据的分类信息，将两组数据映射到了另外一个坐标轴上，使得数据更易区分了（在低维上就可以区分，减少了运算量）。

### 局部线性嵌入

### 拉普拉斯特征映射
