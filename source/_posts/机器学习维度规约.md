---
title: 机器学习维度规约
comments: true
toc: true
mathjax2: true
categories: Algorithm
tags:
  - python
  - security
  - help
  - database
  - Data Mining
keywords: 'Hexo, Blog'
date: 2017-01-10 20:31:17
---
任何分类或者回归方法的复杂度都依赖于样本的规模N和输入的维度d，而特征选择和特征提取是较少复杂性的有效手段。也就是所谓的维度规约
 <!--more-->
## 特征选择
从d个维度中找出提供最多信息的k个维度

### 子集选择
d个变量有2的d次方个可能的子集，在如此多可能情况下如何选择最佳子集呢？

用F代表输入维的特征xi的集合，E（F）表示当只使用F中的输入时，在验证样本上出现的误差。
#### 向前选择
思想：从空集开始，逐渐添加变量，每次添加一个使得误差降低最多的变量，直到进一步的添加不会降低误差或降低很少
算法：从F=空开始，在每一步，对于所有可能的输入xi，训练我们的模型并在验证集上计算E（F∪xi），然后我们选择导致最小误差的输入xi，并将xi加入到F。也可以设定一个阈值，提前结束算法。

#### 向后选择
思想：从所有变量集合中，逐个删除变量，每一步删除降低误差最多的变量，直到进一步的删除不会降低误差或者降低很少
算法：同上相反

这两种情况下，误差检测都应该在不同于训练集的验证集上进行，因为我们需要检测泛化准确率，使用更多的特征一般会有更低的训练误差，但不一定有更低的验证误差。
开销很大，且不能保证找到最佳子集（因为xi和xj本身可能效果不好，但是合起来可能误差降低很多），是个局部最优化算法。可以通过一次增加多个甚至还可以去掉之前增加的特征的方法提高找到最佳子集的可能性，但也增加可复杂度
在无用特征大于有用特征时，选择向前搜索，反之选择向后搜索

人脸识别这样的应用中，特征选择不是降维的好方法，携带脸部识别信息的是许多像素值的组合，因此可以用特征提取方法来实现。

## 特征提取
找出k个维的新集合，这些维是原来d个维的组合，最主要的方法包括主成分分析(PCA)和线性判别分析(LDA)，分别为非监督的和监督的。

### 主成分分析（非监督方法）
需要最大化的准则是方差
主成分是这样的wi，样本投影到wi上之后最分散，使得样本点之间的差别变得最明显。为了得到唯一解且使该方向成为最重要因素，我们要求||wi||=1.
也就是输入样本的协方差矩阵的最大特征值对应的特征向量
对离群点很敏感
