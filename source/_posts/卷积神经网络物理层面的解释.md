---
title: 卷积神经网络物理层面的解释
comments: true
toc: true
mathjax2: true
categories: [artificial-intelligence,deep-learning]
tags: [deep-learning,Neural Networks]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
date: 2017-05-09 14:45:31
---
Deep Learning是全部深度学习算法的总称，CNN是深度学习算法在图像处理领域的一个应用。CNN已经成为众多科学领域的研究热点之一，特别是在模式分类领域，由于该网络避免了对图像的复杂前期预处理，可以直接输入原始图像，因而得到了更为广泛的应用。
 <!--more-->
 一般地，CNN的基本结构包括两层:
 其一为特征提取层，每个神经元的输入与前一层的局部接受域相连，并提取该局部的特征。一旦该局部特征被提取后，它与其它特征间的位置关系也随之确定下来；
 其二是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射是一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数。卷积神经网络中的每一个卷积层都紧跟着一个用来求局部平均与二次提取的计算层，这种特有的两次特征提取结构减小了特征分辨率。

# 步骤：
## 卷积也叫滤波
特征图的大小（卷积特征）由下面三个参数控制，我们需要在卷积前确定它们：
深度（Depth）：深度对应的是卷积操作所需的滤波器个数。
步长（Stride）：步长是我们在输入矩阵上滑动滤波矩阵的像素数。
零填充（Zero-padding）：有时，在输入矩阵的边缘使用零值进行填充，这样我们就可以对输入图像矩阵的边缘进行滤波。
## 非线性处理
ReLU 是一个元素级别的操作（应用到各个像素），并将特征图中的所有小于 0 的像素值设置为零。ReLU 的目的是在 ConvNet 中引入非线性，因为在大部分的我们希望 ConvNet 学习的实际数据是非线性的（卷积是一个线性操作——元素级别的矩阵相乘和相加，所以我们需要通过使用非线性函数 ReLU 来引入非线性。
## 池化
空间池化（Spatial Pooling）（也叫做亚采用或者下采样）降低了各个特征图的维度，但可以保持大部分重要的信息。空间池化有下面几种方式：最大化、平均化、加和等等。
池化函数可以逐渐降低输入表示的空间尺度。特别地，池化：
使输入表示（特征维度）变得更小，并且网络中的参数和计算的数量更加可控的减小，因此，可以控制过拟合
使网络对于输入图像中更小的变化、冗余和变换变得不变性（输入的微小冗余将不会改变池化的输出——因为我们在局部邻域中使用了最大化/平均值的操作。
帮助我们获取图像最大程度上的尺度不变性（准确的词是“不变性”）。它非常的强大，因为我们可以检测图像中的物体，无论它们位置在哪里

## 全连接
卷积和池化层的输出表示了输入图像的高级特征。全连接层的目的是为了使用这些特征把输入图像基于训练数据集进行分类。
除了分类，添加一个全连接层也（一般）是学习这些特征的非线性组合的简单方法。从卷积和池化层得到的大多数特征可能对分类任务有效，但这些特征的组合可能会更好。
从全连接层得到的输出概率和为 1。这个可以在输出层使用 softmax 作为激活函数进行保证。softmax 函数输入一个任意大于 0 值的矢量，并把它们转换为零一之间的数值矢量，其和为一。

# 训练过程
反向传播
完整的卷积网络的训练过程可以总结如下：
1. 第一步：我们初始化所有的滤波器，使用随机值设置参数/权重
2. 第二步：网络接收一张训练图像作为输入，通过前向传播过程（卷积、ReLU 和池化操作，以及全连接层的前向传播），找到各个类的输出概率
3. 第三步：在输出层计算总误差（计算各类的和）
Total Error = ∑  ½ (target probability – output probability) ²
4. 第四步：使用反向传播算法，根据网络的权重计算误差的梯度，并使用梯度下降算法更新所有滤波器的值/权重以及参数的值，使输出误差最小化
权重的更新与它们对总误差的占比有关
当同样的图像再次作为输入，这时的输出概率可能会是 [0.1, 0.1, 0.7, 0.1]，这就与目标矢量 [0, 0, 1, 0] 更接近了
这表明网络已经通过调节权重/滤波器，可以正确对这张特定图像的分类，这样输出的误差就减小了
像滤波器数量、滤波器大小、网络结构等这样的参数，在第一步前都是固定的，在训练过程中保持不变——仅仅是滤波器矩阵的值和连接权重在更新
5. 第五步：对训练数据中所有的图像重复步骤 1 ~ 4

上面的这些步骤可以训练 ConvNet —— 这本质上意味着对于训练数据集中的图像，ConvNet 在更新了所有权重和参数后，已经优化为可以对这些图像进行正确分类。
当一张新的（未见过的）图像作为 ConvNet 的输入，网络将会再次进行前向传播过程，并输出各个类别的概率（对于新的图像，输出概率是使用已经在前面训练样本上优化分类的参数进行计算的）。如果我们的训练数据集非常的大，网络将会（有希望）对新的图像有很好的泛化，并把它们分到正确的类别中去。

注 1: 上面的步骤已经简化，也避免了数学详情，只为提供训练过程的直观内容。
注 2:在上面的例子中我们使用了两组卷积和池化层。然而，这些操作可以在一个 ConvNet 中重复多次。实际上，现在有些表现最好的 ConvNet 拥有多达十几层的卷积和池化层！同时，每次卷积层后面不一定要有池化层。
