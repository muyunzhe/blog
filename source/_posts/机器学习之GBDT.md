---
title: 机器学习之GBDT
comments: true
toc: true
mathjax2: true
date: 2017-12-01 16:03:49
categories: [artificial-intelligence,machine-learning]
tags: [machine-learning,classification]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
---
GBDT的基本思想是“积跬步以至千里”！也就是说我每次都只学习一点，然后一步步的接近最终要预测的值（完全是gradient的思想~）。换句话来说，我们先用一个初始值来学习一棵决策树，叶子处可以得到预测的值，以及预测之后的残差，然后后面的决策树就要基于前面决策树的残差来学习，直到预测值和真实值的残差为零。最后对于测试样本的预测值，就是前面许多棵决策树预测值的累加。整个过程都是每次学习一点（真实值的一部分），最后累加，所以叫做“积跬步以至千里”！
 <!--more-->
## 基于参差的版本
网上流传了一个预测年龄的例子，例子举的很好，当时解释的过程没有结合GBDT的工作过程，这里我们改进一下：
训练集：（A, 14岁）、（B，16岁）、（C, 24岁）、（D, 26岁）；
训练数据的均值：20岁；   （这个很重要，因为GBDT与i开始需要设置预测的均值，这样后面才会有残差！）
决策树的个数：2棵；
每个样本的特征有两个：购物金额是否小于1K；经常去百度提问还是回答；

开始GBDT学习了~
首先，输入初值20岁，根据第一个特征（具体选择哪些特征可以根据信息增益来计算选择），可以把4个样本分成两类，一类是购物金额<=1K，一类是>=1K的。假如这个时候我们就停止了第一棵树的学习，这时我们就可以统计一下每个叶子中包含哪些样本，这些样本的均值是多少，因为这个时候的均值就要作为所有被分到这个叶子的样本的预测值了。比如AB被分到左叶子，CD被分到右叶子，那么预测的结果就是：AB都是15岁，CD都是25岁。和他们的实际值一看，结果发现出现的残差，ABCD的残差分别是-1, 1, -1, 1。这个残差，我们要作为后面第二棵决策树的学习样本。
![](/img/cancha1.png)
然后学习第二棵决策树，我们把第一棵的残差样本（A, -1岁）、（B，1岁）、（C, -1岁）、（D, 1岁）输入。此时我们选择的特征是经常去百度提问还是回答。这个时候我们又可以得到两部分，一部分是AC组成了左叶子，另一部分是BD组成的右叶子。那么，经过计算可知左叶子均值为-1，右叶子均值为1. 那么第二棵数的预测结果就是AC都是-1，BD都是1. 我们再来计算一下此时的残差，发现ABCD的残差都是0！停止学习~
![](/img/cancha2.png)

这样，我们的两棵决策树就都学习好了。进入测试环节：
测试样本：请预测一个购物金额为3k，经常去百度问淘宝相关问题的女生的年龄~
我们提取2个特征：购物金额3k，经常去百度上面问问题；
第一棵树 —> 购物金额大于1k —> 右叶子，初步说明这个女生25岁
第二棵树 —> 经常去百度提问 —> 左叶子，说明这个女生的残差为-1；
叠加前面每棵树得到的结果：25-1=24岁，最终预测结果为24岁~

### 这个版本的核心思路：
每个回归树学习前面树的残差，并且用shrinkage把学习到的结果大步变小步，不断迭代学习。其中的代价函数是常见的均方差。
### 其基本做法就是：
先学习一个回归树，然后“真实值-预测值*shrinkage”求此时的残差，把这个残差作为目标值，学习下一个回归树，继续求残差……直到建立的回归树的数目达到一定要求或者残差能够容忍，停止学习。
### 优缺点
我们知道，残差是预测值和目标值的差值，这个版本是把残差作为全局最优的绝对方向来学习。
这个版本更加适用于回归问题，线性和非线性的均可，而且在设定了阈值之后还可以有分类的功能。
当时该版本使用残差，很难处理纯回归以外的问题。
Shrinkage和梯度下降法中学习步长alpha的关系。shrinkage设小了只会让学习更慢，设大了就等于没设，它适用于所有增量迭代求解问题；而Gradient的步长设小了容易陷入局部最优点，设大了容易不收敛。它仅用于用梯度下降求解。这两者其实没太大关系。

## 基于梯度的版本
只要建立的代价函数能够求导，那么就可以使用版本二的GBDT算法，例如LambdaMART学习排序算法。
我们可以先明确一个思路，就是梯度版本的GBDT是用多类分类Multi-class classification 的思想来实现的，或者可以说GBDT的这个版本融入到多类分类中可以更好的掌握。

### 算法
![基于梯度的算法](/img/基于梯度的算法.png)

基于残差的版本是把残差作为全局方向，偏向于回归的应用。
而基于梯度的版本是把代价函数的梯度方向作为更新的方向，适用范围更广。
如果使用Logistic函数作为代价函数，那么其梯度形式和残差的形式类似，这个就说明两个版本之间是紧密联系的，虽然实现的思路不同，但是总体的目的是一样的。或者说残差版本是梯度版本的一个特例，当代价函数换成其余的函数，梯度的版本仍是适用的。
