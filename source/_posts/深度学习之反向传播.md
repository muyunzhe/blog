---
title: 深度学习之反向传播
comments: true
toc: true
mathjax2: true
date: 2017-08-25 14:02:41
categories: [artificial-intelligence,deep-learning]
tags: [deep-learning,Neural Networks]
keywords: '牧云者,人工智能,云计算,数据挖掘,hexo,blog'
---
BP算法实际上是一种近似的最优解决方案，背后的原理仍然是梯度下降，但为了解决上述困难，其方案是将多层转变为一层接一层的优化：只优化一层的参数是可以得到显式梯度下降表达式的；而顺序呢必须反过来才能保证可工作——由输出层开始优化前一层的参数，然后优化再前一层……跑一遍下来，那所有的参数都优化过一次了。
 <!--more-->
 https://www.zhihu.com/question/24827633/answer/29189075

## maxpool 怎么做反向传播？
对于max-pooling，在前向计算时，是选取的每个2*2区域中的最大值，这里需要记录下最大值在每个小区域中的位置。在反向传播时，只有那个最大值对下一层有贡献，所以将残差传递到该最大值的位置，区域内其他2*2-1=3个位置置零。
对于mean-pooling，我们需要把残差平均分成2*2=4份，传递到前边小区域的4个单元即可。
